---
title: "<center>Ethics|Final Project|Part I</center>"
author: "<center>Aldo Adriazola, Avisek Choudhury, Kait Arlond<br> East Section</center>"
date: "<center>06/13/2020</center>"
output:
  pdf_document: 
    toc: yes
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
In 2012, Minerva High School in Pittsburgh, Pennsylvania was struggling with its high drop out rates at 9% and low (on-time) graduation rates of 55%.  The school's principal and board will respond to the problem with a data-driven solution aimed at getting their students back on track and gaining insight into the why behind the high dropout rates. This solution will use machine learning to identify predictors of student disengagement as a proxy for potential drop out. Flagging at risk students will enable the teachers, counselors, administrators to respond accordingly with new interventions, meetings with school counselors and/or parents, new incentive structures, etc in the hopes of reversing the high dropout rates. 


## Data Import and Cleaning
Upon import of the data, we converted all string columns into factors utilizing R's stringAsFactors argument in the read.csv function. We then removed the student ID field because it will not be used in this analysis. We converted all factors into numeric and converted the dropped field back into a factor for use as the response variable in our modeling efforts. 

In an effort to normalize our data. we rescaled our resulting data using the following logic:
ùëß=(ùë•‚àímin(ùë•))/(max(ùë•)‚àímin(ùë•))



```{r readData, message=FALSE, warning=FALSE, error=TRUE}
#Load the libraries
library(dplyr)
library(readr)
library(tidyverse)
library(class)
library(caret)
library(rpart)
library(partykit)
library(randomForest)
library(e1071)

#Read the csv file
case3data <- read.csv("~/Desktop/KA/Notre Dame/2020Summer/Ethics/case3data.csv",stringsAsFactors = TRUE)

# remove studentID and move dropped to the first column
case3data <- case3data %>%
    select(-studentID) %>% select(dropped,everything())
 
# convert all factors to numeric
case3data <-case3data %>% mutate_if(is.factor,as.numeric)

# convert dropped to a factor
case3data$dropped <- as.factor(case3data$dropped)

#Print the summary
#case3data %>% summary

```

```{r dataCleansing, warning=FALSE, error=TRUE, message=FALSE}
# First, rescale the data
# create the rescaling function we have been using thus far
rescale_x <- function(x){(x-min(x))/(max(x)-min(x))}
# create a copy of the df
rescaled_df <- case3data
# apply the rescale function to all columns except dropped
rescaled_df[2:14] <- sapply(rescaled_df[2:14],rescale_x)
# confirm rescaling worked correctly
# all rescaled vars should be within [0,1]
summary(rescaled_df)
# Now split the data
# set the seed to Notre Dame's founding year
set.seed(1842)
# determine the number of rows in the dataframe
n <- nrow(rescaled_df)
# get a list of 20% of the rows in combined to use as indices
test_idx <- sample.int(n, size = round(0.2 * n))
# set the the training data to be those rows not matching the index list
training <- rescaled_df[-test_idx,]
# set the the test data to be those rows matching the index list
testing <- rescaled_df[test_idx,] 

```


## Model Building


### Logistic Regression

```{r logisticRegression, warning=FALSE, error=TRUE, message=FALSE}
# set the seed for consistent results
set.seed(1842)

# Set up the resampling, here repeated CV
tr <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
#Note that trace is a parameter sent to the underlying modeling function
logistic_model <- train(dropped ~ ., 
                        data = training, 
                        method = "glm", 
                        family = "binomial", 
                        trControl = tr, 
                        trace = FALSE)

#Check the Final Model
logistic_model$finalModel

#Predict
testing$logistic_pred <- predict(logistic_model, newdata = testing)

# create the confusion matrix using the table function
confusion_logistic <- table(Predicted =testing$logistic_pred,
                        Actual=testing$dropped)

# show the accuracy of the decision tree
cat("Overall accuracy of prediction:\t", 
    sum(diag(confusion_logistic)/nrow(testing)) %>% 
      round(4),"\n")
```

### Decision Tree

```{r descTree, warning=FALSE, error=TRUE, message=FALSE}
# set the seed for consistent results
set.seed(1842)

# Define the formula
form <- paste(names(rescaled_df[1]),paste(names(rescaled_df[-1]), collapse='+'),sep = '~') %>% 
  as.formula()

# show the formula
form

# Generate the Decision tree
diag.tree <- rpart(form, data=training)
# Print the Tree CP
printcp(diag.tree)
# Plotting the Tree CP
plotcp(diag.tree)
# Partykit plot of the Tree
plot(as.party(diag.tree))
```

```{r pruneTree, warning=FALSE, error=TRUE, message=FALSE}
diag.new.tree <- prune(diag.tree, cp = 0.029)
# Partykit plot of the Pruned Tree
plot(as.party(diag.new.tree))
```

```{r treePredict, warning=FALSE, error=TRUE, message=FALSE}
# use the decision tree created above to predict values in the test data 
# and then store the results
testing$tree_predict <- predict(diag.new.tree, 
                                newdata=testing, 
                                type="class")
# create the confusion matrix using the table function
confusion_tree <- table(Predicted =testing$tree_predict,
                        Actual=testing$dropped)

# Print a legend
cat("0 = Not Dropped Out, 1 = Dropped Out\n\n")
# Print the confusion matrix
confusion_tree

# show the accuracy of the decision tree
cat("Overall accuracy of prediction:\t", 
    sum(diag(confusion_tree)/nrow(testing)) %>% 
      round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying Not Dropped Out as Dropped Out:\t", 
    (confusion_tree[1,2] / 
       (confusion_tree[1,1] + confusion_tree[1,2])) %>% 
      round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying Dropped Out as Not Dropped Out:\t", 
    (confusion_tree[2,1] / 
       (confusion_tree[2,1] + confusion_tree[2,2])) %>% 
      round(4),"\n")
```

### Random Forest

```{r randForest, warning=FALSE, error=TRUE, message=FALSE}
# set the seed for consistent results
set.seed(1842)
# Generate the Random Forest
diag.forest <- randomForest(form, mtry = 3, 
                            ntree = 500, 
                            data=training, 
                            na.action = na.roughfix)
# Print the Random Forest
diag.forest
```

Let's generate the importance table of the predictors from the random forest model.

```{r rfImportance, warning=FALSE, error=TRUE, message=FALSE}
# Importance of Variables
randomForest::importance(diag.forest) %>%
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename(VarName = rowname) %>% 
  arrange(desc(MeanDecreaseGini))
```


We use the forest to predict the diagnosis in the test data.  We show the overall accuracy of the prediction.  Then we show the overall accuracy of the classifier with this value of K as well as the misclassification rates.


```{r rfPredict, warning=FALSE, error=TRUE, message=FALSE}
# use the Random Forest created above to predict values in the test data 
# and then store the results
testing$rf_pred <- predict(diag.forest, 
                           newdata=testing, 
                           type="class")
# create the confusion matrix using the table function
confusion_rf <- table(Predicted=testing$rf_pred,
                        Actual=testing$dropped)

# Print a legend
cat("0 = Not Dropped Out, 1 = Dropped Out\n\n")
# Print the confusion matrix
confusion_rf

# show the accuracy of the decision tree
cat("Overall accuracy of prediction:\t", 
    sum(diag(confusion_rf)/nrow(testing)) %>% 
      round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying Not Dropped Out as Dropped Out:\t", 
    (confusion_rf[1,2] / 
       (confusion_rf[1,1] + confusion_rf[1,2])) %>% 
      round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying Dropped Out as Not Dropped Out:\t", 
    (confusion_rf[2,1] / 
       (confusion_rf[2,1] + confusion_rf[2,2])) %>% 
      round(4),"\n")
```



### K-Nearest Neighbors 

```{r knnModel, warning=FALSE, error=TRUE, message=FALSE}
# Choose a value for K that is equal to the square root of n,
# the number of observations in the training set
k_try = sqrt(nrow(training))
k_try
# We'll use 21 as our value of K
diag_knn <- knn(training[2:14], 
                   testing[2:14], 
                   cl = training$dropped, 
                   k=119)

# create the confusion matrix using the table function
confusion_knn <- table(Predicted=diag_knn,
                        Actual=testing$dropped)

# Print a legend
cat("0 = Not Dropped Out, 1 = Dropped Out\n\n")
# Print the confusion matrix
confusion_knn

# show the accuracy of the decision tree
cat("Overall accuracy of prediction:\t", 
    sum(diag(confusion_knn)/nrow(testing)) %>% 
      round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying Not Dropped Out as Dropped Out:\t", 
    (confusion_knn[1,2] / 
       (confusion_knn[1,1] + confusion_knn[1,2])) %>% 
      round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying Dropped Out as Not Dropped Out:\t", 
    (confusion_knn[2,1] / 
       (confusion_knn[2,1] + confusion_knn[2,2])) %>% 
      round(4),"\n")

```

Let's tune K to see if we can get better accuracy


We will use the caret package "train" function to see if a diffent value of K results in higher accuracy.  This code block will step through odd numbers from 1 to 99 as values of K.  If a better value of N is found, then we will use that value instead.

```{r knnCaret, warning=FALSE, error=TRUE, message=FALSE}
# set the seed for consistent results
set.seed(1842)
# set the train control to use 5-fold cross validation
# choosing 5-fold as a good middle ground
trControl <- trainControl(method  = "cv",
                          number  = 5)
# find the best knn fit using values of K of all odd numbers from 1 to 99
knn_fit <- train(dropped ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = c((1:50)*2 - 1)),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = training)
# Print the Model
knn_fit
```


Revising our solution to use K = 5

We use the optimized value of K that was determined in the step above. Then we show the overall accuracy of the classifier with this optimized value of K as well as the misclassifcation rates.

```{r tuneKnn, warning=FALSE, error=TRUE, message=FALSE}
# We'll use the value obtained above as our value of K
diag_knn_opt <- knn(training[2:14],testing[2:14],
                    cl=training$dropped,k=knn_fit$bestTune)
# create and display the confusion matrix
confusion_knn_opt <- table(Predicted = diag_knn_opt, 
                           Actual = testing$dropped)
# Print a legend
cat("0 = Not Dropped Out, 1 = Dropped Out\n\n")
# Print the confusion matrix
confusion_knn_opt

# show the accuracy of the decision tree
cat("Overall accuracy of prediction:\t", 
    sum(diag(confusion_knn_opt)/nrow(testing)) %>% 
      round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying Not Dropped Out as Dropped Out:\t", 
    (confusion_knn_opt[1,2] / 
       (confusion_knn_opt[1,1] + confusion_knn_opt[1,2])) %>% 
      round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying Dropped Out as Not Dropped Out:\t", 
    (confusion_knn_opt[2,1] / 
       (confusion_knn_opt[2,1] + confusion_knn_opt[2,2])) %>% 
      round(4),"\n")
```

## Summary & Conclusion


